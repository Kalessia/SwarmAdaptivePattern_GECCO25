{
    "evolutionary_settings info": "'nb_evals' is the max budjet of allowed evaluations for the entire learning process. flags distance in [MSE, SSIM, CLIP] (add check init)",
    "evolutionary_settings": {
        "nb_runs": 11,
        "nb_evals": 14000,
        "env_name info": "env_name in ['flag_automata', 'sliding_puzzle_incremental']",
        "env_name": "sliding_puzzle_incremental",
        "sliding_puzzle_incremental": {
            "sliding_puzzle_incremental info": "Incremental learning refers to an evolutionary process that changes of parameters after a defined generation. Here, we modify the density of a sliding_puzzle environment, given by sliding_puzzle_incremental_nb_deletions_ticks[0] in the 1st setup and sliding_puzzle_incremental_nb_deletions_ticks[1] in the 2nd. The switch generation is deducted from 'switch_eval'. Either 'units' or 'percent' must be null. 2 values should be different... check",
            "sliding_puzzle_incremental_switch_eval": 0,
            "sliding_puzzle_incremental_nb_deletions_units": null,
            "sliding_puzzle_incremental_nb_deletions_percent": [0.0, 0.9],
            "sliding_puzzle_incremental_proba_move": 0.9
        },
        "flags_distance_mode": "CLIP" 
    },

    "nn_controller info": "The agent's controller is a forward neural network. 'nb_neuronsPerInputs' is >= 4. 'hidden_layers' lists the nb of neurons per hidden layer. One biais neuron is added to the input and each hidden layer. Weights (individual) values are in ]-inf, +inf[",
    "nn_controller": {
        "nb_neuronsPerInputs": 4,
        "hidden_layers": [2],
        "nb_neuronsPerOutputs": 2
    },

    "grid info": "flag_patterns in ['two-bands', 'three-bands', 'centered-disc', 'not-centered-disc', 'centered-half-discs', 'not-centered-half-discs']. Set init_cell_state_value as a scalar or null (random).",
    "grid": {
        "flag_pattern": "two-bands",
        "grid_nb_rows": 16,
        "grid_nb_cols": 16,
        "init_cell_state_value": 0.0
    },

    "environment info": "Time step is the total length of the development flag task. Time window is the interval of steps where we compute the mean of each step fitness. Time window is in [time_window_start (step), time_window_end (step)].",
    "environment": {
        "time_steps": 50,
        "time_window_start": 30,
        "time_window_end": 50
    },

    "learning_options info": "learning_options in ['learning_random_async_update_states', 'learning_random_init_states', 'learning_with_noise']",
    "learning_options": {
        "learning_random_async_update_states": {
            "learning_random_async_update_states info": "false: agents states are updated in random grid-positions order and asynchronous time, meaning that the updated state value of a chosen agent is used to compute the state of a not yet updated neighbor (realistic robotic setup). true: in grid-positions order, each agent computes its new state using pre-update neighbors state values, then all new agents states are updated synchronously. In both options, all agents states are updated.",
            "learning_random_async_update_states_bool": false
        },
        "learning_random_init_states": {
            "learning_random_init_states info": "false: the initial state of each agent is a fixed vector of init_cell_state_value. true: the initial state of each agent is a list of random values (normal distribution in [0,1[ ).",
            "learning_random_init_states_bool": false
        },
        "learning_with_noise": {
            "learning_with_noise info": "Each newly computed agent' state is perturbed by a noise N(0, learning_with_noise_std, len_state)",
            "learning_with_noise_bool": false,
            "learning_with_noise_std": 1.0e-5
        }
    },

    "verbose_debug": false,

    "with_parallelization_bool": true,
    "with_parallelization_nb_free_cores": 0
}